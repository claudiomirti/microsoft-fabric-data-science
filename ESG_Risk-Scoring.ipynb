{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microsoft Fabric Exercises \n",
    "# Author: Claudio Mirti, Microsoft\n",
    "# Disclaimer: This is a random data set and used for demo purpose only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random ESG data for the last 6 months focusing on Europe\n",
    "start_date = datetime.now() - timedelta(days=180)\n",
    "end_date = datetime.now()\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "companies = ['Company A', 'Company B', 'Company C', 'Company D', 'Company E']\n",
    "regions = ['France', 'Italy', 'Switzerland', 'Spain', 'UK']\n",
    "risk_scores = [random.uniform(0, 10) for _ in range(len(dates))]\n",
    "carbon_footprints = [random.uniform(0, 1000) for _ in range(len(dates))]\n",
    "esg_sentiment_scores = [random.uniform(-1, 1) for _ in range(len(dates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the ESG data\n",
    "data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Company': random.choices(companies, k=len(dates)),\n",
    "    'Region': random.choices(regions, k=len(dates)),\n",
    "    'Risk_Score': risk_scores,\n",
    "    'Carbon_Footprint': carbon_footprints,\n",
    "    'ESG_Sentiment_Score': esg_sentiment_scores\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table to showcase company and risk scoring model over different regions\n",
    "pivot_table = data.pivot_table(index='Company', columns='Region', values='Risk_Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap for the pivot table\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(data=pivot_table, cmap='YlGnBu', annot=True, fmt=\".2f\", cbar=True)\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Company')\n",
    "plt.title('Company and Risk Scoring Model across Regions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "sparkDF=spark.createDataFrame(data) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write Spark Table to Delta Lake\n",
    "sparkDF.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + \"ESG\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
