{"cells":[{"cell_type":"code","execution_count":null,"id":"d4af0c33-cacc-489f-a9c6-32a63515d6e1","metadata":{},"outputs":[],"source":["# Microsoft Fabric Exercises \n","# Author: Claudio Mirti, Microsoft\n","# Disclaimer: This is a random data set and used for demo purpose only."]},{"cell_type":"code","execution_count":null,"id":"af4c552c-5e2b-4283-8194-463103e1d998","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from pyspark.sql import SparkSession\n","\n","# Generate random revenue data\n","np.random.seed(42)\n","countries = ['Europe', 'America', 'Asia']\n","num_months = 12\n","\n","revenue_data = []\n","for country in countries:\n","    revenue = np.cumsum(np.random.randint(100, 1000, num_months))\n","    revenue_data.append(revenue)\n","\n","# Create a DataFrame\n","df = pd.DataFrame(np.transpose(revenue_data), columns=countries)\n","\n","# Add predictive forecast data\n","forecast_months = 6\n","forecast_data = []\n","for country in countries:\n","    last_month_revenue = df[country].iloc[-1]\n","    forecast = np.cumsum(np.random.randint(100, 1000, forecast_months)) + last_month_revenue\n","    forecast_data.append(forecast)\n","\n","df_forecast = pd.DataFrame(np.transpose(forecast_data), columns=countries)\n","df = pd.concat([df, df_forecast])\n","\n","# Create line chart with ongoing revenue and predictive forecast\n","plt.figure(figsize=(12, 6))\n","sns.lineplot(data=df, dashes=False)\n","\n","# Set chart labels and title\n","plt.xlabel('Months')\n","plt.ylabel('Revenue')\n","plt.title('Revenue Forecast by Country')\n","\n","# Set legend\n","plt.legend(title='Country', loc='upper left')\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"e265c78d-407d-4eab-a13f-f579ae0bc9c1","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Create PySpark DataFrame from Pandas\n","sparkDF=spark.createDataFrame(df_forecast) \n","sparkDF.printSchema()\n","sparkDF.show()"]},{"cell_type":"code","execution_count":null,"id":"db4f7dc8-0895-4001-9a4a-0343bce884e6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# write spark table\n","sparkDF.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + \"MFI\") "]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"notebook_environment":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"72e7e415-3aec-43e5-b34b-ad4db2e9db48","default_lakehouse_name":"LakeCLOUDIO","default_lakehouse_workspace_id":"5e33c7c9-b6f9-4955-824c-ffc3d15bf720","known_lakehouses":[{"id":"72e7e415-3aec-43e5-b34b-ad4db2e9db48"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
